{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb894c73-d733-4eae-a08a-a03dbbe74f21",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c344af-dbc5-4010-a2c4-c5f47a30e2ef",
   "metadata": {},
   "source": [
    "For this hackathon we have provided features derived from the ESM protein language model combined with fitness scores for each sequence. We have created a PyTorch dataloader that provides you with this data. Your model may choose to only use some of the data (we leave this up to you). This notebook explores the structure of the data. For additional example code for training and evaluating your models look at the following python file:\n",
    "\n",
    "`src/train.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3090523-bb96-4562-b737-24571a088b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T12:31:07.396357Z",
     "iopub.status.busy": "2024-04-05T12:31:07.395975Z",
     "iopub.status.idle": "2024-04-05T12:31:08.941454Z",
     "shell.execute_reply": "2024-04-05T12:31:08.940735Z",
     "shell.execute_reply.started": "2024-04-05T12:31:07.396327Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "sys.path.append('src') \n",
    "from src.data_loader import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d47984-2be0-41a3-b432-01ddb9e206ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T12:31:08.943619Z",
     "iopub.status.busy": "2024-04-05T12:31:08.943036Z",
     "iopub.status.idle": "2024-04-05T12:31:08.960393Z",
     "shell.execute_reply": "2024-04-05T12:31:08.959795Z",
     "shell.execute_reply.started": "2024-04-05T12:31:08.943582Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_path = \"data/HUMAN\"\n",
    "data_loader = get_dataloader(experiment_path, folds=[1,2,3,4], batch_size=3)\n",
    "type(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87eb0e17-77e2-4511-bae9-64d7eb8d1b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T12:31:08.961964Z",
     "iopub.status.busy": "2024-04-05T12:31:08.961499Z",
     "iopub.status.idle": "2024-04-05T12:31:08.975893Z",
     "shell.execute_reply": "2024-04-05T12:31:08.974806Z",
     "shell.execute_reply.started": "2024-04-05T12:31:08.961929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>mutated_sequence</th>\n",
       "      <th>DMS_score</th>\n",
       "      <th>DMS_score_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A101C</td>\n",
       "      <td>MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...</td>\n",
       "      <td>0.573154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A101F</td>\n",
       "      <td>MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...</td>\n",
       "      <td>0.765705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A101G</td>\n",
       "      <td>MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...</td>\n",
       "      <td>-2.460507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A101H</td>\n",
       "      <td>MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...</td>\n",
       "      <td>-2.230238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A101I</td>\n",
       "      <td>MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...</td>\n",
       "      <td>1.122181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mutant                                   mutated_sequence  DMS_score  \\\n",
       "0  A101C  MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...   0.573154   \n",
       "1  A101F  MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...   0.765705   \n",
       "2  A101G  MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...  -2.460507   \n",
       "3  A101H  MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...  -2.230238   \n",
       "4  A101I  MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTND...   1.122181   \n",
       "\n",
       "   DMS_score_bin  \n",
       "0              1  \n",
       "1              1  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's look at the metadata\n",
    "df = pd.read_csv(\"data/HUMAN/HUMAN.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fcba6-1d4c-40f3-a963-229869e33344",
   "metadata": {},
   "source": [
    "We can see that the metadata dataframe contains sequences for the same protein, each one with a single mutation. The mutation is specified by the first column A101C means that in position 101 amino acid A (alanine) was replaced with C (cysteine). The DMS_score is the value we are trying to predict. DMS_score_bin is a binary version of the DMS_score where 1 is considered success and 0 is considered failure. The threshold used for binarization varies between datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872eb2c7-c4fe-4ae0-b779-928619e20077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T12:31:08.979657Z",
     "iopub.status.busy": "2024-04-05T12:31:08.977314Z",
     "iopub.status.idle": "2024-04-05T12:31:08.997356Z",
     "shell.execute_reply": "2024-04-05T12:31:08.996156Z",
     "shell.execute_reply.started": "2024-04-05T12:31:08.979602Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type returned by the dataloader is <class 'dict'>\n",
      "The keys of the dataloader are dict_keys(['embedding', 'mutant', 'DMS_score', 'mutant_sequence', 'logits', 'wt_logits', 'wt_embedding'])\n"
     ]
    }
   ],
   "source": [
    "# next let's see what data is returned by the dataloader:\n",
    "for batch in data_loader:\n",
    "    print(f\"The type returned by the dataloader is {type(batch)}\")\n",
    "    print(f\"The keys of the dataloader are {batch.keys()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37f232a-c822-4ee2-b71c-d695a432ffab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T12:31:08.998911Z",
     "iopub.status.busy": "2024-04-05T12:31:08.998545Z",
     "iopub.status.idle": "2024-04-05T12:31:09.007607Z",
     "shell.execute_reply": "2024-04-05T12:31:09.006737Z",
     "shell.execute_reply.started": "2024-04-05T12:31:08.998878Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape: torch.Size([3, 152, 1280]) \n",
      "\n",
      "wt_embedding shape: torch.Size([3, 152, 1280]) \n",
      "\n",
      "mutants: ['V103M', 'I96W', 'I107W'] \n",
      "\n",
      "DMS_score: tensor([ 0.4536, -0.7084, -0.0655]) \n",
      "\n",
      "mutant_sequence: ['MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTNDTHKRDTYAATPRAHEVSEISVRTVYPPEEETGERVQLAHHFSEPEITLIIFGVMAGMIGTILLISYGIRRLIKKSPSDVKPLPSPDTDVPLSSVEIENPETSDQ', 'MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTNDTHKRDTYAATPRAHEVSEISVRTVYPPEEETGERVQLAHHFSEPEITLIWFGVMAGVIGTILLISYGIRRLIKKSPSDVKPLPSPDTDVPLSSVEIENPETSDQ', 'MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTNDTHKRDTYAATPRAHEVSEISVRTVYPPEEETGERVQLAHHFSEPEITLIIFGVMAGVIGTWLLISYGIRRLIKKSPSDVKPLPSPDTDVPLSSVEIENPETSDQ'] \n",
      "\n",
      "logits shape: torch.Size([3, 152, 33]) \n",
      "\n",
      "wt_logits shape: torch.Size([3, 152, 33]) \n"
     ]
    }
   ],
   "source": [
    "# note that the first dimension is the batch size\n",
    "print(\"embedding shape:\", batch['embedding'].shape, '\\n')\n",
    "print(\"wt_embedding shape:\", batch['wt_embedding'].shape, '\\n')\n",
    "print(\"mutants:\", batch['mutant'], '\\n')\n",
    "print(\"DMS_score:\", batch[\"DMS_score\"], '\\n')\n",
    "print(\"mutant_sequence:\", batch[\"mutant_sequence\"], '\\n')\n",
    "print(\"logits shape:\", batch[\"logits\"].shape, '\\n')\n",
    "print(\"wt_logits shape:\", batch[\"wt_logits\"].shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555bcba2-b4b2-4bd9-887d-2203e1dd2b50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ESM embeddings as features for predicting fitness\n",
    "[ESM is a protein language model](https://github.com/facebookresearch/esm) which is used to create embedded representations of proteins that can then be used as features for downstream tasks (like we are doing here).\n",
    "\n",
    "The embeddings shape is composed of \\[batch_size, sequence_length + 2, ESM_embedding_size\\] the embedding feature is likely to be the most useful for our purposes and you may choose to not use any of the other features.\n",
    "We add 2 to the sequence length because the ESM model adds a special token at the start and end of the sequence.\n",
    "\n",
    "wt stands for wild-type: it means the canonical sequence of the protein (without any mutation applied). The wild type features for a particular protein never change. They are repeated within each batch according to the batch size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d082a19-928f-47b6-a3f4-7cc78bc9d52e",
   "metadata": {},
   "source": [
    "## Basic embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bc8f57-4faa-4fe6-89dc-874199ed9f8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T12:31:09.009758Z",
     "iopub.status.busy": "2024-04-05T12:31:09.008890Z",
     "iopub.status.idle": "2024-04-05T12:31:09.041989Z",
     "shell.execute_reply": "2024-04-05T12:31:09.041102Z",
     "shell.execute_reply.started": "2024-04-05T12:31:09.009723Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 152, 1280])\n",
      "Shape after layer1: torch.Size([3, 152, 256])\n",
      "Shape after summing over sequence dim.: torch.Size([3, 256])\n",
      "Output shape: torch.Size([3, 1])\n",
      "loss = 39.85172653198242\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1280, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, data: dict):\n",
    "        \"\"\"\n",
    "        :param data: Dictionary containing ['embedding', 'mutant', 'mutant_sequence',\n",
    "                                                'logits', 'wt_logits', 'wt_embedding']\n",
    "        :return: predicted DMS score\n",
    "        \"\"\"\n",
    "        x = data['embedding']\n",
    "        print('Input shape:', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        print('Shape after layer1:', x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = torch.sum(x, dim=1)\n",
    "        print('Shape after summing over sequence dim.:',x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print('Output shape:', x.shape)\n",
    "        return x\n",
    "\n",
    "model = EmbeddingModel()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "data_loader = get_dataloader(experiment_path, folds=[1,2,3,4], batch_size=3)\n",
    "for batch in data_loader:\n",
    "    preds = model(batch)\n",
    "    preds = preds.squeeze()\n",
    "    labels = batch[\"DMS_score\"]\n",
    "    labels = labels.squeeze()\n",
    "    loss = loss_fn(preds, labels)\n",
    "    print(f'loss = {loss}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23daaf26-42b1-48ef-a5e7-5716ea305d20",
   "metadata": {},
   "source": [
    "## ESM likelihood scores (logits)\n",
    "\n",
    "ESM was trained to do multi-class classification: for each amino acid in the sequence the model is trained to predict what was the input token. There are 33 possible input tokens (see cell below). 20 of these tokens represent the standard amino acids and the additional tokens are used to represent things such as the start, end, masked token, padding, unknown etc.\n",
    "\n",
    "It has been shown that if ESM predicts a mutation to be unlikely then this is more likely to be a disease causing mutation and mutations that ESM deems acceptable are more likely to be associated with higher protein stability or other fitness scores.\n",
    "\n",
    "If your model does not need logits and or wt features you can set return_logits=False, return_wt=false when calling: `get_dataloader()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f57b720-9fc5-4211-b43b-229617c212a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T12:31:09.043565Z",
     "iopub.status.busy": "2024-04-05T12:31:09.043154Z",
     "iopub.status.idle": "2024-04-05T12:31:09.050114Z",
     "shell.execute_reply": "2024-04-05T12:31:09.049289Z",
     "shell.execute_reply.started": "2024-04-05T12:31:09.043530Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Each of the 33 position indexes in the logits output represents \n",
    "# the ESM-predicted likelihood of different tokens.\n",
    "\n",
    "tok_to_idx = {'<cls>': 0, \n",
    "              '<pad>': 1, \n",
    "              '<eos>': 2, \n",
    "              '<unk>': 3, \n",
    "              'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, \n",
    "              'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, \n",
    "              'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, \n",
    "              'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, \n",
    "              'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, \n",
    "              '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
    "\n",
    "idx_to_tok = {v:k for k,v in tok_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ee417-a668-413d-932a-4ef6d357b1ce",
   "metadata": {},
   "source": [
    "Running the cell below we can see that the ESM model generally assigns the highest probability to the token that was observed at input, however occasionally it predicts a different token. One way to evaluate how preferable a mutation is to take the likelihood ratio of the mutant residue and wildtype residue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2137ca-7bcf-4168-9414-e02fc4d43edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T12:31:32.498026Z",
     "iopub.status.busy": "2024-04-05T12:31:32.497648Z",
     "iopub.status.idle": "2024-04-05T12:31:32.515890Z",
     "shell.execute_reply": "2024-04-05T12:31:32.515241Z",
     "shell.execute_reply.started": "2024-04-05T12:31:32.498000Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual residue: <cls>, most likely residue: <cls> match?: True\n",
      "actual residue: M, most likely residue: M match?: True\n",
      "actual residue: Y, most likely residue: Y match?: True\n",
      "actual residue: G, most likely residue: G match?: True\n",
      "actual residue: K, most likely residue: K match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: F, most likely residue: F match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: A, most likely residue: A match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: G, most likely residue: G match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: A, most likely residue: A match?: True\n",
      "actual residue: M, most likely residue: M match?: True\n",
      "actual residue: H, most likely residue: H match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: K, most likely residue: K match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: Y, most likely residue: Y match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: Q, most likely residue: Q match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: N, most likely residue: N match?: True\n",
      "actual residue: D, most likely residue: D match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: H, most likely residue: H match?: True\n",
      "actual residue: K, most likely residue: K match?: True\n",
      "actual residue: R, most likely residue: R match?: True\n",
      "actual residue: D, most likely residue: D match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: Y, most likely residue: T match?: False\n",
      "actual residue: A, most likely residue: A match?: True\n",
      "actual residue: A, most likely residue: A match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: R, most likely residue: R match?: True\n",
      "actual residue: A, most likely residue: A match?: True\n",
      "actual residue: H, most likely residue: H match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: R, most likely residue: R match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: Y, most likely residue: Y match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: G, most likely residue: G match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: R, most likely residue: R match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: Q, most likely residue: Q match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: A, most likely residue: A match?: True\n",
      "actual residue: H, most likely residue: H match?: True\n",
      "actual residue: H, most likely residue: H match?: True\n",
      "actual residue: F, most likely residue: F match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: F, most likely residue: F match?: True\n",
      "actual residue: G, most likely residue: G match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: M, most likely residue: M match?: True\n",
      "actual residue: A, most likely residue: A match?: True\n",
      "actual residue: G, most likely residue: G match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: G, most likely residue: G match?: True\n",
      "actual residue: P, most likely residue: L match?: False\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: Y, most likely residue: Y match?: True\n",
      "actual residue: G, most likely residue: G match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: R, most likely residue: R match?: True\n",
      "actual residue: R, most likely residue: R match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: K, most likely residue: K match?: True\n",
      "actual residue: K, most likely residue: K match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: D, most likely residue: D match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: K, most likely residue: K match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: D, most likely residue: D match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: D, most likely residue: D match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: L, most likely residue: L match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: V, most likely residue: V match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: I, most likely residue: I match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: N, most likely residue: N match?: True\n",
      "actual residue: P, most likely residue: P match?: True\n",
      "actual residue: E, most likely residue: E match?: True\n",
      "actual residue: T, most likely residue: T match?: True\n",
      "actual residue: S, most likely residue: S match?: True\n",
      "actual residue: D, most likely residue: D match?: True\n",
      "actual residue: Q, most likely residue: Q match?: True\n",
      "actual residue: <eos>, most likely residue: <eos> match?: True\n"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "mut_seq = batch['mutant_sequence'][batch_idx]\n",
    "# every sequence has a <cls> token at the start and a <eos> token at the end\n",
    "n_embeddings = len(mut_seq) + 2\n",
    "for seq_idx in range(n_embeddings):\n",
    "    # get the position index with the highest logit, lookup which token is represented by that position\n",
    "    most_likely_token = idx_to_tok[np.argmax(batch[\"logits\"][batch_idx, seq_idx,:].numpy())]\n",
    "    if seq_idx == 0:\n",
    "        actual_residue = \"<cls>\"\n",
    "    elif seq_idx == n_embeddings - 1:\n",
    "        actual_residue = \"<eos>\"\n",
    "    else:\n",
    "        actual_residue = mut_seq[seq_idx - 1]\n",
    "    print(f'actual residue: {actual_residue}, most likely residue: {most_likely_token} match?: {actual_residue==most_likely_token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60301e-e917-4bbf-966e-0d184894ccd0",
   "metadata": {},
   "source": [
    "## Basic likelihood model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfa3d67-fa84-477b-84f6-0d8a5192d1bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T12:33:32.954653Z",
     "iopub.status.busy": "2024-04-05T12:33:32.954270Z",
     "iopub.status.idle": "2024-04-05T12:33:32.976936Z",
     "shell.execute_reply": "2024-04-05T12:33:32.976006Z",
     "shell.execute_reply.started": "2024-04-05T12:33:32.954623Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutants: ['T106P', 'L94W', 'L94C']\n",
      "logits torch.Size([3, 152, 33])\n",
      "Positions: [106, 94, 94]\n",
      "AA tokens: [14, 22, 23]\n",
      "Mutant logits: tensor([ 1.1531, -0.8305, -0.9094])\n",
      "loss = 6.579700946807861\n"
     ]
    }
   ],
   "source": [
    "class LikelihoodModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model returns the logit (un-normalised likelihood) of the \n",
    "    mutant residue as an estimator of the fitness score.\n",
    "    It doesn't have any learnable parameters. In general, \n",
    "    likelihood ratios MT/WT are probably better than doing this.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LikelihoodModel, self).__init__()\n",
    "        \n",
    "    def get_mutated_position_idx(self, data):\n",
    "        return [int(m[1:-1]) for m in data['mutant']]\n",
    "    \n",
    "    def get_mutant_aa_token_idx(self, data):\n",
    "        return [tok_to_idx[m[-1]] for m in data['mutant']]\n",
    "        \n",
    "\n",
    "    def forward(self, data: dict):\n",
    "        print('Mutants:', data['mutant'])\n",
    "        print('logits', data['logits'].shape)\n",
    "        mutated_position_idx = self.get_mutated_position_idx(data)\n",
    "        print('Positions:', mutated_position_idx)\n",
    "        mutant_token_idx = self.get_mutant_aa_token_idx(data)\n",
    "        print('AA tokens:', mutant_token_idx)\n",
    "        batch_indices = torch.arange(data['logits'].size(0))\n",
    "        mutant_logit = data['logits'][batch_indices, mutated_position_idx, mutant_token_idx]\n",
    "        print('Mutant logits:', mutant_logit)\n",
    "        return mutant_logit\n",
    "\n",
    "model = LikelihoodModel()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "data_loader = get_dataloader(experiment_path, folds=[1,2,3,4], batch_size=3)\n",
    "for batch in data_loader:\n",
    "    preds = model(batch)\n",
    "    preds = preds.squeeze()\n",
    "    labels = batch[\"DMS_score\"]\n",
    "    labels = labels.squeeze()\n",
    "    loss = loss_fn(preds, labels)\n",
    "    print(f'loss = {loss}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0b323-a8b9-4d61-9d5f-5aff69b20bfa",
   "metadata": {},
   "source": [
    "## Train, validation, test splits using folds\n",
    "\n",
    "The data loader has a parameter called 'folds' which controls which sequences are returned by the data loader.\n",
    "\n",
    "This allows us to split the sequences into 5 folds which can be used for training, validation and testing.\n",
    "\n",
    "Each sequence is assigned to one of 5 folds based on taking its mutation position modulo 5. The data loader will only return sequences which have a mutation that are assigned to one of its folds.\n",
    "\n",
    "In the final evaluation we will create 3 data loaders:\n",
    "\n",
    "`train_loader = get_dataloader(experiment_path, folds=[1,2,3])`\n",
    "\n",
    "`val_loader = get_dataloader(experiment_path, folds=[4])`\n",
    "\n",
    "`test_loader = get_dataloader(experiment_path, folds=[5])`\n",
    "\n",
    "`train_loader` and `val_loader` will be passed to your customized `train_model()` function like so:\n",
    "\n",
    "`train_model(model, train_loader, val_loader)`\n",
    "\n",
    "After training, we will evaluate your model using our `evaluate_model()` function which you must not change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
